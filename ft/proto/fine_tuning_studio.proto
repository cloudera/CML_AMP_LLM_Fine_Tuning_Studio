/**

CML LLM Fine Tuning Studio

This is the protobuf definition used at the API
surface of the Fine Tuning Studio (FTS) application. Given that 
this may integrate in the future as a first-class citizen of CML, 
we are prepending FTS to our objects to make it abundantly clear
that these model metadata definitions, etc., are different
from the CML model metadata definitions.

*/

syntax = "proto3";

package fine_tuning_studio;

/**
-----------------------
gRPC Service Definition
-----------------------
*/

// gRPC service representation of the Fine Tuning Studio app. This 
// presents an API surface to interact with the gRPC server for requests.
service FineTuningStudio {
  
  // Dataset operations
  rpc ListDatasets (ListDatasetsRequest) returns (ListDatasetsResponse) {}
  rpc GetDataset (GetDatasetRequest) returns (GetDatasetResponse) {}
  rpc AddDataset (AddDatasetRequest) returns (AddDatasetResponse) {}
  rpc RemoveDataset (RemoveDatasetRequest) returns (RemoveDatasetResponse) {}
  
  // Model operations
  rpc ListModels (ListModelsRequest) returns (ListModelsResponse) {}
  rpc GetModel (GetModelRequest) returns (GetModelResponse) {}
  rpc AddModel (AddModelRequest) returns (AddModelResponse) {}
  rpc ExportModel (ExportModelRequest) returns (ExportModelResponse) {}
  rpc RemoveModel (RemoveModelRequest) returns (RemoveModelResponse) {}

  // Model adapter operations
  rpc ListAdapters (ListAdaptersRequest) returns (ListAdaptersResponse) {}
  rpc GetAdapter (GetAdapterRequest) returns (GetAdapterResponse) {}
  rpc AddAdapter (AddAdapterRequest) returns (AddAdapterResponse) {}
  rpc RemoveAdapter (RemoveAdapterRequest) returns (RemoveAdapterResponse) {}
  
  // Prompt operations
  rpc ListPrompts (ListPromptsRequest) returns (ListPromptsResponse) {}
  rpc GetPrompt (GetPromptRequest) returns (GetPromptResponse) {}
  rpc AddPrompt (AddPromptRequest) returns (AddPromptResponse) {}
  rpc RemovePrompt (RemovePromptRequest) returns (RemovePromptResponse) {}

  // Training Jobs
  rpc ListFineTuningJobs (ListFineTuningJobsRequest) returns (ListFineTuningJobsResponse) {}
  rpc GetFineTuningJob (GetFineTuningJobRequest) returns (GetFineTuningJobResponse) {}
  rpc StartFineTuningJob (StartFineTuningJobRequest) returns (StartFineTuningJobResponse) {}
  rpc RemoveFineTuningJob (RemoveFineTuningJobRequest) returns (RemoveFineTuningJobResponse) {}

  // Evaluation Jobs
  rpc ListEvaluationJobs (ListEvaluationJobsRequest) returns (ListEvaluationJobsResponse) {}
  rpc GetEvaluationJob (GetEvaluationJobRequest) returns (GetEvaluationJobResponse) {}
  rpc StartEvaluationJob (StartEvaluationJobRequest) returns (StartEvaluationJobResponse) {}
  rpc RemoveEvaluationJob (RemoveEvaluationJobRequest) returns (RemoveEvaluationJobResponse) {}

  // Quantization Configs
  rpc ListConfigs (ListConfigsRequest) returns (ListConfigsResponse) {}
  rpc GetConfig (GetConfigRequest) returns (GetConfigResponse) {}
  rpc AddConfig (AddConfigRequest) returns (AddConfigResponse) {}
  rpc RemoveConfig (RemoveConfigRequest) returns (RemoveConfigResponse) {}

  // Other general operations
  rpc GetAppState (GetAppStateRequest) returns (GetAppStateResponse) {}
}


/**
-------------------------------------------
gRPC protobuf Request/Response Definitions
-------------------------------------------
*/

// Dataset controls
message ListDatasetsRequest {

}
message ListDatasetsResponse {
  repeated DatasetMetadata datasets = 1;
}
message GetDatasetRequest {
  string id = 1;
}
message GetDatasetResponse {
  DatasetMetadata dataset = 1;
}
message AddDatasetRequest {
  // Type of dataset to be imported.
  DatasetType type = 1;

  // If this is a huggingface dataset, the huggingface name
  // should be provided.
  string huggingface_name = 2;

  // If this is a project-relative dataset, then add project
  // location
  string location = 3;
}
message AddDatasetResponse {
  DatasetMetadata dataset = 1;
}
message RemoveDatasetRequest {
  string id = 1;
  bool remove_prompts = 2;
}
message RemoveDatasetResponse {

}


// Model controls
message ListModelsRequest {

}
message ListModelsResponse {
  repeated ModelMetadata models = 1;
}
message GetModelRequest {
  string id = 1;
}
message GetModelResponse {
  ModelMetadata model = 1;
}
message AddModelRequest {
  // type of model to import. This affects how the model
  // is loaded and how metadata is extracted for the model. 
  ModelType type = 1; 

  // Name of the huggingface model. This is the full huggingface
  // model name used to identify the model on HF hub.
  string huggingface_name = 2;

  // Model ID of the model in the model registry of the workspace.
  // Used when importing models from model registries.
  string model_registry_id = 3;
}
message AddModelResponse {
  ModelMetadata model = 1;
}
// Export a model out of the FTS app ecosystem. 
message ExportModelRequest {

  // Type of export model operation to perform.
  ModelType type = 1; 

  // Model ID that should be exported
  string model_id = 2;

  // Trained adapter that is to also be
  // exported (optional). Depending on the model
  // export type, any PEFT adapter weights may be
  // merged into the base model.
  string adapter_id = 3;

  // Human-friendly name to give to the exported
  // model. Might not be used if only exporting
  // model to a file output (for example, ONNX output)
  string model_name = 4;

  // Export output artifact location for export types
  // that require file-writing to project files.
  string artifact_location = 5;

  // model description for those model export
  // types that allow for descriptions.
  string model_description = 6;

}
message ExportModelResponse {

  // If a model is exported, this typically means a new
  // model is created and can technically be imported into
  // the FTS app. For this reason, the ModelMetadata object
  // can be used to store this information.
  ModelMetadata model = 1;
}
message RemoveModelRequest {
  string id = 1;
}
message RemoveModelResponse {
  
}



// Adapter controls
message ListAdaptersRequest {

}
message ListAdaptersResponse {
  repeated AdapterMetadata adapters = 1;
}
message GetAdapterRequest {
  string id = 1;
}
message GetAdapterResponse {
  AdapterMetadata adapter = 1;
}
message AddAdapterRequest {
  AdapterMetadata adapter = 1;
}
message AddAdapterResponse {
  AdapterMetadata adapter = 1;
}
message RemoveAdapterRequest {
  string id = 1;
}
message RemoveAdapterResponse {
  
}


// Prompt controls
message ListPromptsRequest {

}
message ListPromptsResponse {
  repeated PromptMetadata prompts = 1;
}
message GetPromptRequest {
  string id = 1;
}
message GetPromptResponse {
  PromptMetadata prompt = 1;
}
message AddPromptRequest {
  // For now, we are requiring the entire
  // prompt metadata to be passed (in fact, this is
  // how the original API layer worked anyway, so
  // this is fine)
  PromptMetadata prompt = 1;
}
message AddPromptResponse {
  PromptMetadata prompt = 1;
}
message RemovePromptRequest {
  string id = 1;
}
message RemovePromptResponse {
  
}


// FineTuningJob controls
message ListFineTuningJobsRequest {

}
message ListFineTuningJobsResponse {
  repeated FineTuningJobMetadata fine_tuning_jobs = 1;
}
message GetFineTuningJobRequest {
  string id = 1;
}
message GetFineTuningJobResponse {
  FineTuningJobMetadata fine_tuning_job = 1;
}
message StartFineTuningJobRequest {
  // Human-friendly identifier for the name of the output adapter.
  string adapter_name = 1;

  // The model ID of the base model that should be used as a
  // base for the fine tuning job.
  string base_model_id = 2;

  // The dataset that will be used to perform the training.
  // This dataset ID is the App-specific ID.
  string dataset_id = 3;

  // The prompt that will be used for training. This is
  // tied to the dataset for now, but that won't necessarily
  // be a many-to-one relationship in the future.
  string prompt_id = 4;

  // Number of workers to use for this fine-tuning job.
  int32 num_workers = 5;

  // Automatically add the trained job as an adapter to the app.
  bool auto_add_adapter = 7;

  // Number of epochs to run during fine-tuning.
  int32 num_epochs = 8;

  // Learning rate to use during fine-tuning.
  float learning_rate = 9;

  // Number of CPUs to allocate for this job.
  int32 cpu = 10;

  // Number of GPUs to allocate for this job.  
  int32 gpu = 11;

  // Amount of memory to allocate for this job (e.g., '16Gi').
  int32 memory = 12;

  // Optional dataset test split to split the dataset into a training
  // dataset and an eval dataset. Evaluation datasets are used at epoch boundaries
  // during training to compute metrics and compte loss again.
  float train_test_split = 13;

  // Bits and bytes config used for the model layers.
  string model_bnb_config_id = 14;

  // Bits and bytes config used for the adapter. For 
  // most use cases, this should be the same id as 
  // for the model, but technically a model can have
  // a different quantization config for training 
  // than an adapter.
  string adapter_bnb_config_id = 15;

  string training_arguments_config_id = 16;

  string lora_config_id = 17;
}

message StartFineTuningJobResponse {
  FineTuningJobMetadata fine_tuning_job = 1;
}
message RemoveFineTuningJobRequest {
  string id = 1;
}
message RemoveFineTuningJobResponse {
  
}


// EvaluationJob controls
message ListEvaluationJobsRequest {

}
message ListEvaluationJobsResponse {
  repeated EvaluationJobMetadata evaluation_jobs = 1;
}
message GetEvaluationJobRequest {
  string id = 1;
}
message GetEvaluationJobResponse {
  EvaluationJobMetadata evaluation_job = 1;
}
message StartEvaluationJobRequest {

  // Type of EvaluationJob to start
  EvaluationJobType type = 1;

  // The model ID of the base model that should be used as a
  // base for the job.
  string base_model_id = 2;

  // The dataset that will be used to perform the training.
  // This dataset ID is the App-specific ID.
  string dataset_id = 3;

  // Adapter ID of the adapter for this job
  string adapter_id = 4;

  // Number of CPUs to allocate for this job.
  int32 cpu = 5;

  // Number of GPUs to allocate for this job.
  int32 gpu = 6;

  // Amount of memory to allocate for this job (e.g., '16Gi').
  int32 memory = 7;

  // Bits and bytes config used for the model layers.
  string model_bnb_config_id = 14;

  // Bits and bytes config used for the adapter. For 
  // most use cases, this should be the same id as 
  // for the model, but technically a model can have
  // a different quantization config for training 
  // than an adapter.
  string adapter_bnb_config_id = 15;

  // Id for the generation args config.
  string generation_config_id = 16;
}
message StartEvaluationJobResponse {
  EvaluationJobMetadata job = 1;
}
message RemoveEvaluationJobRequest {
  string id = 1;
}
message RemoveEvaluationJobResponse {
  
}


message ListConfigsRequest {

  // Optionally only return a specific config type.
  ConfigType type = 1;
}

message ListConfigsResponse {
  repeated ConfigMetadata configs = 1;
}

message GetConfigRequest {
  string id = 1;
}

message GetConfigResponse {
  ConfigMetadata config = 1;
}

message AddConfigRequest {
  ConfigType type = 1;
  string config = 2;
}

message AddConfigResponse {
  // check for id matching, etc.
  ConfigMetadata config = 1;
}

message RemoveConfigRequest {
  string id = 1;
}

message RemoveConfigResponse {

}


// App state controls 
message GetAppStateRequest {
  
  // In the future, we may want to implement some form of
  // RBAC for state. But for now, as an AMP, we don't need
  // this.
  string user = 1;
}
message GetAppStateResponse {
  AppState state = 1;
}



/**
-------------------------
protobuf enum definitions
-------------------------
*/


// Type of dataset. This type determines how a 
// dataset is extracted or loaded into memory when
// running fine-tuning jobs.
//
// Note that enum values use C++ scoping rules, meaning that enum values are siblings of their type,
// not children of it.  Therefore, enum names must be unique within "fine_tuning_studio", not just within 
// their enums. Hence the prefix of the enum.
enum DatasetType {

  // Dataset is stored on HF hub and can be 
  // downloaded whenever needed.
  DATASET_TYPE_HUGGINGFACE = 0;

  // Dataset is locally stored in project files
  // and can be referenced by a dataset location.
  DATASET_TYPE_PROJECT = 1;
}

// Type of model. This AMP currently supports
// loading models in huggingface, from CML's
// model registry, and finally from local project
// files within a CML workspace.
enum ModelType {

  // Huggingface model.
  MODEL_TYPE_HUGGINGFACE = 0;

  // Model imported from project files.
  //
  // TODO: determine a way to extract model framework from the content
  // of the provided file directory (or by other parameters
  // in the model metadata request)
  MODEL_TYPE_PROJECT = 1;

  // Model was imported from CML Model Registry.
  MODEL_TYPE_MODEL_REGISTRY = 2;

}

// The model framework used for this model. Depending on
// the model type (i.e. HF, project, registry), handling
// the model may be different (for example, for local projects,
// we should specify a file/packaging format for ONNX models.)
// 
// TODO: for the most part, this AMP only supports pytorch models
// at this time. we should support other frameworks.
enum ModelFrameworkType {

  // Pytorch
  MODEL_FRAMEWORK_TYPE_PYTORCH = 0;

  // Tensorflow
  MODEL_FRAMEWORK_TYPE_TENSORFLOW = 1;

  // ONNX
  MODEL_FRAMEWORK_TYPE_ONNX = 2;

}

// Type of PEFT adapter.
enum AdapterType {

  // Project-relative PEFT adapter imported from project
  // files, probably created after a fine-tuning
  // job was ran in our app.
  ADAPTER_TYPE_PROJECT = 0;

  // Huggingface-stored adapter that can be pulled
  // down from HF hub.
  ADAPTER_TYPE_HUGGINGFACE = 1;

  // Adapter stored within the CML model registry.
  ADAPTER_TYPE_MODEL_REGISTRY = 2;
}


enum JobStatus {
  JOB_STATUS_SCHEDULED = 0;
  JOB_STATUS_RUNNING = 1;
  JOB_STATUS_SUCCESS = 2;
  JOB_STATUS_FAILURE = 3;
}

enum PromptType {
  PROMPT_TYPE_IN_PLACE = 0;
}


enum EvaluationJobType {
  EVALUATION_JOB_TYPE_MLFLOW = 0;
}


// Configuration type
enum ConfigType {
  CONFIG_TYPE_UNKNOWN = 0;
  CONFIG_TYPE_TRAINING_ARGUMENTS = 1;
  CONFIG_TYPE_BITSANDBYTES_CONFIG = 2;
  CONFIG_TYPE_GENERATION_CONFIG = 3;
  CONFIG_TYPE_LORA_CONFIG = 4;
}

/**
-----------------------------
protobuf datatype definitions
-----------------------------
*/


// Metadata about a dataset that is being tracked in FTS. 
message DatasetMetadata {

  // FTS id of the dataset.
  string id = 1;

  // Type of the dataset.
  DatasetType type = 2;

  // human-readable name of the dataset. 
  string name = 3;
  
  // description of the dataset.
  string description = 4;

  // canonical huggingface dataset name (can be used to find
  // huggingface hub if this is a huggingface dataset)
  string huggingface_name = 5;

  // Project-relative location of the dataset that is
  // loaded into the app's state, if this is a project dataset.
  string location = 6;

  // list of features in the dataset.
  repeated string features = 7;

}


// Metadata about a registered model. This can
// apply right now to both adapters as well as
// models (TODO: need to check this logic)
message RegisteredModelMetadata {

  // Model ID of the registered model.
  string cml_registered_model_id = 1;

  // MLFlow experiment ID. This allows us to extract individual
  // model artifacts from the model registry, for example.
  string mlflow_experiment_id = 2;

  // MLFlow run ID tied to this specific model artifact. This is used
  // to extract individual model artifacts from MLFlow.
  string mlflow_run_id = 3;

}


// Metadata about a model that is loaded into the FTS
// application.
message ModelMetadata {
  
  // Global identifier for models.
  //
  // For the purpose of this
  // AMP application, during local ML model loading & inference,
  // model IDs are random unique identifiers that have no
  // significance within the CML ecosystem. Evenutally when this
  // AMP is integrated with CML model registry, we will ideally
  // be able to have a more significant model ID.
  string id = 1;

  // Type of model. This type affects the source of where models
  // are loaded from.
  ModelType type = 2;

  // framework of the model.
  ModelFrameworkType framework = 3;

  // human-friendly name for the model.
  string name = 4;

  // Name of the huggingface model. This is the human-readable
  // model name that can be used to identify a huggingface model
  // on HF hub.
  string huggingface_model_name = 5;

  // Location of the model if it is a local project model.
  string location = 6;

  // Metadata on the registered model with CML model registry,
  // if this model is a model registry type.
  RegisteredModelMetadata registered_model = 7;
}



message AdapterMetadata {

  // Unique ID of the PEFT adapter.
  string id = 1;

  // Type of model adapter.
  AdapterType type = 2;

  // Human friendly name of the adapter for tracking.
  string name = 3;

  // Corresponding model ID that this adapter is designed for. This is the
  // model ID in the FT app.
  string model_id = 4;

  // Project-relative directory where the PEFT adapter data is stored.

  // When training with HF/TRL libraries, a typical output directory
  // for PEFT adapters will contain files like:
  // * adapter_config.json
  // * adapter_model.bin
  
  // This dataclass currently just stores the location of the PEFT adapter
  // in the local directory which can then be used to load an adapter.
  string location = 5;

  // Huggingface PEFT adapter name (identifier used to find
  // the adapter on HF hub).
  string huggingface_name = 6;

  // Job ID of the job that was used to train/create this adapter. This is
  // used to determine if an adapter is completely trained or not.
  string job_id = 7;

  // Prompt ID of the prompt that was used to train this adapter.
  string prompt_id = 8;

  // Adapters should eventually have support in CML model registry. This metadata
  // will be stored here for adapters in case this is available.
  RegisteredModelMetadata registered_model = 9;
}


message PromptMetadata {

  // Unique ID of the prompt in question.
  string id = 1;

  // Human-friendly name of this prompt template
  // for use-cases elsewhere
  string name = 2;

  // ID of the dataset that uses this prompt.
  // This dataset should contain column names
  // that correspond to the items that are
  // in the list of slots.
  string dataset_id = 3;

  // Python formatted prompt string template.
  string prompt_template = 4;
}


message WorkerProps {
  int32 num_cpu = 1;
  int32 num_memory = 2;
  int32 num_gpu = 3;
}




message FineTuningJobMetadata {

  // Unique job identifier of the job. For some job implementations (local
  // fine tuning with the AMP), this job ID does not specifically have a
  // CML counterpart or significance in the CDP ecosystem.
  string job_id = 1;

  // The model ID of the base model that should be used as a
  // base for the fine tuning job.
  string base_model_id = 2;

  // The dataset that will be used to perform the training.
  // This dataset ID is the App-specific ID.
  string dataset_id = 3;

  // The prompt that will be used for training. This is
  // tied to the dataset for now, but that won't necessarily
  // be a many-to-one relationship in the future.
  string prompt_id = 4;

  // Number of workers to use for this fine-tuning job.
  int32 num_workers = 5;

  // CML identifier for the created CML job.
  string cml_job_id = 6;

  // Adapter ID of the adapter that this job is training.
  string adapter_id = 7;

  // Properties of each worker that will be spawned up.
  WorkerProps worker_props = 8;

  // Number of epochs to run during fine-tuning.
  int32 num_epochs = 9;

  // Learning rate to use during fine-tuning.
  float learning_rate = 10;

  // Output directory for the adapter
  string out_dir = 11;

  // Training arguments for the run.
  string training_arguments_config_id = 12;

  // Bits and bytes config used for the model layers.
  string model_bnb_config_id = 13;

  // Bits and bytes config used for the adapter. For 
  // most use cases, this should be the same id as 
  // for the model, but technically a model can have
  // a different quantization config for training 
  // than an adapter.
  string adapter_bnb_config_id = 14;

  string lora_config_id = 15;
}


message ConfigMetadata {
  string id = 1;

  // type of configuration
  ConfigType type = 2;

  // Serialized (json) representation of the config. This
  // can be passed directly into any of the config types
  // at runtime, such as transformers.TrainingArguments(),
  // peft.LoraConfig(), and so on.
  string config = 3;
}


message EvaluationJobMetadata {

  // Unique job identifier of the job. For some job implementations (local
  // fine tuning with the AMP), this job ID does not specifically have a
  // CML counterpart or significance in the CDP ecosystem.
  string job_id = 1;

  // CML identifier for the created CML job.
  string cml_job_id = 2;

  // The model ID of the base model that should be used as a
  // base for the fine tuning job.
  string base_model_id = 3;

  // The dataset that will be used to perform the training.
  // This dataset ID is the App-specific ID.
  string dataset_id = 4;

  // Number of workers to use for this evaluation job.
  int32 num_workers = 5;

  // Adapter ID of the adapter that this job is training.
  string adapter_id = 6;

  // Properties of each worker that will be spawned up.
  WorkerProps worker_props = 7;

  // Resulting directory of evaluation
  string evaluation_dir = 8;

  // BnB config of the model
  string model_bnb_config_id = 9;

  // BnB config of the adapter
  string adapter_bnb_config_id = 10;

  // Generation argument configs. 
  string generation_config_id = 11;
}

message AppState {
  repeated DatasetMetadata datasets = 1;
  repeated ModelMetadata models = 2;
  repeated FineTuningJobMetadata fine_tuning_jobs = 3;
  repeated EvaluationJobMetadata evaluation_jobs = 4;
  repeated PromptMetadata prompts = 5;
  repeated AdapterMetadata adapters = 6;
  repeated ConfigMetadata configs = 7;
}